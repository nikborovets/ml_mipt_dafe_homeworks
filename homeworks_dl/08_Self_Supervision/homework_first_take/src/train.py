"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Prototypical Networks.
–í–∫–ª—é—á–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Å TensorBoard –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
"""

import os
import argparse
from typing import Optional
import yaml
from datetime import datetime

import numpy as np
import torch
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

from .data import read_images, extract_sample
from .model import load_protonet_conv


def train(model, optimizer, train_x: np.ndarray, train_y: np.ndarray, 
          n_way: int, n_support: int, n_query: int, 
          max_epoch: int, epoch_size: int,
          writer: Optional[SummaryWriter] = None,
          save_path: str = "models/protonet.pt") -> None:
    """
    # from omniglot_hw_from_ipynb.py: –æ–±—É—á–∞–µ—Ç protonet —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º TensorBoard –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    Trains the protonet
    Args:
      model: –º–æ–¥–µ–ª—å ProtoNet
      optimizer: –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
      train_x (np.array): images of training set
      train_y (np.array): labels of training set
      n_way (int): number of classes in a classification task
      n_support (int): number of labeled examples per class in the support set
      n_query (int): number of labeled examples per class in the query set
      max_epoch (int): max epochs to train on
      epoch_size (int): episodes per epoch
      writer: TensorBoard writer –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
      save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    """
    # divide the learning rate by 2 at each epoch, as suggested in paper
    scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)
    epoch = 0  # epochs done so far
    stop = False  # status to know when to stop
    best_acc = 0.0  # –ª—É—á—à–∞—è accuracy –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    while epoch < max_epoch and not stop:
        running_loss = 0.0
        running_acc = 0.0

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
        pbar = tqdm(range(epoch_size), desc=f"Epoch {epoch + 1}/{max_epoch}")
        
        for episode in pbar:
            sample = extract_sample(n_way, n_support, n_query, train_x, train_y)
            optimizer.zero_grad()
            loss, output = model.set_forward_loss(sample)
            running_loss += output['loss']
            running_acc += output['acc']
            loss.backward()
            optimizer.step()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            pbar.set_postfix({
                'Loss': f"{output['loss']:.4f}",
                'Acc': f"{output['acc']:.4f}"
            })
        
        epoch_loss = running_loss / epoch_size
        epoch_acc = running_acc / epoch_size
        
        print(f'Epoch {epoch+1:d} -- Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
        
        # TensorBoard –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if writer is not None:
            writer.add_scalar('Train/Loss', epoch_loss, epoch)
            writer.add_scalar('Train/Accuracy', epoch_acc, epoch)
            writer.add_scalar('Train/Learning_Rate', scheduler.get_last_lr()[0], epoch)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if epoch_acc > best_acc:
            best_acc = epoch_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_acc': best_acc,
                'loss': epoch_loss,
            }, save_path)
            print(f"Saved best model with accuracy: {best_acc:.4f}")
        
        epoch += 1
        scheduler.step()
    
    print(f"Training completed. Best accuracy: {best_acc:.4f}")


def validate(model, val_x: np.ndarray, val_y: np.ndarray,
            n_way: int, n_support: int, n_query: int,
            val_episodes: int = 100) -> tuple:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    Args:
        model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        val_x, val_y: –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        n_way, n_support, n_query: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–ø–∏–∑–æ–¥–∞
        val_episodes: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    Returns:
        tuple: (—Å—Ä–µ–¥–Ω–∏–π loss, —Å—Ä–µ–¥–Ω—è—è accuracy)
    """
    model.eval()
    running_loss = 0.0
    running_acc = 0.0
    
    with torch.no_grad():
        for episode in tqdm(range(val_episodes), desc="Validation"):
            sample = extract_sample(n_way, n_support, n_query, val_x, val_y)
            loss, output = model.set_forward_loss(sample)
            running_loss += output['loss']
            running_acc += output['acc']
    
    model.train()
    avg_loss = running_loss / val_episodes
    avg_acc = running_acc / val_episodes
    
    return avg_loss, avg_acc


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
    parser = argparse.ArgumentParser(description='Train Prototypical Networks')
    parser.add_argument('--config', type=str, default='config.yaml', 
                       help='Path to config file')
    parser.add_argument('--data_dir', type=str, default='data',
                       help='Path to data directory')
    parser.add_argument('--n_way', type=int, default=60,
                       help='Number of classes per episode')
    parser.add_argument('--n_support', type=int, default=5,
                       help='Number of support examples per class')
    parser.add_argument('--n_query', type=int, default=5,
                       help='Number of query examples per class')
    parser.add_argument('--max_epoch', type=int, default=5,
                       help='Maximum number of epochs')
    parser.add_argument('--epoch_size', type=int, default=2000,
                       help='Number of episodes per epoch')
    parser.add_argument('--lr', type=float, default=0.001,
                       help='Learning rate')
    parser.add_argument('--log_dir', type=str, default='runs/omniglot_protonet',
                       help='TensorBoard log directory')
    parser.add_argument('--save_path', type=str, default='models/protonet.pt',
                       help='Path to save the best model')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –µ—Å–ª–∏ –µ—Å—Ç—å (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç –Ω–µ –±—ã–ª –ø–µ—Ä–µ–¥–∞–Ω –≤ CLI)
    if os.path.exists(args.config):
        with open(args.config, 'r') as f:
            config = yaml.safe_load(f)
        
        # –ü–æ–ª—É—á–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        default_parser = argparse.ArgumentParser()
        default_parser.add_argument('--n_way', type=int, default=60)
        default_parser.add_argument('--n_support', type=int, default=5)
        default_parser.add_argument('--n_query', type=int, default=5)
        default_parser.add_argument('--max_epoch', type=int, default=5)
        default_parser.add_argument('--epoch_size', type=int, default=2000)
        default_parser.add_argument('--lr', type=float, default=0.001)
        default_parser.add_argument('--log_dir', type=str, default='runs/omniglot_protonet')
        default_parser.add_argument('--save_path', type=str, default='models/protonet.pt')
        default_args = default_parser.parse_args([])
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–≤–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        for key, value in config.items():
            if hasattr(args, key) and hasattr(default_args, key):
                if getattr(args, key) == getattr(default_args, key):
                    setattr(args, key, value)
    
    print("Loading data...")
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    train_x, train_y = read_images(f'{args.data_dir}/images_background')
    print(f"Loaded training data: {train_x.shape}, {train_y.shape}")
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    print("Creating model...")
    model = load_protonet_conv(
        x_dim=(3, 28, 28),
        hid_dim=64,
        z_dim=64,
    )
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    optimizer = optim.Adam(model.parameters(), lr=args.lr)
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = f"runs/train_{timestamp}"
    print(f"üìä Creating new TensorBoard log directory: {log_dir}")
    
    # –°–æ–∑–¥–∞–µ–º TensorBoard writer
    writer = SummaryWriter(log_dir=log_dir)
    
    print("Starting training...")
    print(f"Device: {model.device}")
    print(f"Parameters: n_way={args.n_way}, n_support={args.n_support}, n_query={args.n_query}")
    print(f"Training: max_epoch={args.max_epoch}, epoch_size={args.epoch_size}")
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    train(
        model=model,
        optimizer=optimizer,
        train_x=train_x,
        train_y=train_y,
        n_way=args.n_way,
        n_support=args.n_support,
        n_query=args.n_query,
        max_epoch=args.max_epoch,
        epoch_size=args.epoch_size,
        writer=writer,
        save_path=args.save_path
    )
    
    writer.close()
    print("Training completed!")


if __name__ == "__main__":
    main() 