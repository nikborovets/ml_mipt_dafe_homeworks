{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whiAANkwhw-I"
      },
      "source": [
        "# MDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XccGClM6h3jh"
      },
      "source": [
        "Когда Инокетений Вальдемарович Носочков приходит на работу, то с вероятностью 50% он хочет пойти в бар, с 10% – покушац, с 20% – спать. Если он ничего не хочет, то он продолжает работать. Когда Кеша пьет в Баре, то в 10% посещений он возвращается на работу и с вероятностью 30% идет спать, но в остальное время продолжает пить. Когда он просыпается, то с вероятностью 40% идет покушац и с вероятностью 60% идет в бар пить дальше. Если вдруг г-н Носочков поел, то с вероятностью 80% он начинает работать, а если не срослось с работой, то он начинает хотеть спать."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS-kwBSRh3mX"
      },
      "source": [
        "Определите вероятности, что наш герой прямо сейчас работает, пьет в баре, спит или ест, при условии, что если Инокетений чего-то хочет, то делает это."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJRO5-9WvM_q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuXufW6hAlo-"
      },
      "source": [
        "# Practice: gym interface and crossentropy method\n",
        "\n",
        "_Reference:_ This notebook is based on Practical RL [week01](https://github.com/yandexdataschool/Practical_RL/tree/master/week01_intro)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftOrVNgg9i-O"
      },
      "source": [
        "## OpenAI Gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg6tdsNtA9hl"
      },
      "source": [
        "Следующие несколько недель мы потратим на изучение алгоритмов, которые помогают в процессе принятия решений. Затем нам понадобится несколько интересных задач для тестирования наших алгоритмов.\n",
        "\n",
        "Вот тут-то и вступает в игру OpenAI Gym. Это библиотека Python, которая охватывает множество классических задач, связанных с принятием решений, включая управление роботами, видеоигры и настольные игры.\n",
        "\n",
        "Итак, вот как это работает:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI0LrNlkY3X9"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "gym.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgUa6Yz89i-Q"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"MountainCar-v0\", render_mode='rgb_array')\n",
        "env.reset()\n",
        "\n",
        "env_screen = env.render()\n",
        "plt.imshow(env_screen)\n",
        "plt.show()\n",
        "\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRTK8yeU9i-R"
      },
      "source": [
        "### Gym interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMWiCMVIVPTI"
      },
      "source": [
        "Существуют три основных метода настройки среды:\n",
        "* `reset()`: возвращает среду в исходное состояние и _return (возвращает ее обратно)_\n",
        "* `render()`: показывает текущее состояние среды (более красочная версия).\n",
        "* `step(a)`: совершаем действие \"a\" и возвращаем \"(new_state, вознаграждение, is_done, информация)\"\n",
        "* `new_state`: новое состояние сразу после совершения действия \"a\".\n",
        "* `reward`: число, представляющее вашу награду за совершение действия \"a\"\n",
        "* `is_done`: значение True, если MDP только что завершился, и значение False, если все еще продолжается\n",
        "* `info`: некоторые дополнительные сведения о том, что только что произошло. Пока не обращайте на это внимания."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhGD7yXr9i-R"
      },
      "outputs": [],
      "source": [
        "state = env.reset()\n",
        "print(\"initial state:\", state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9WYWMRvrl5r"
      },
      "source": [
        "В горной машине наблюдение - это всего лишь две цифры: положение автомобиля и скорость.\n",
        "\n",
        "Давайте выполним действие 2, которое расшифровывается как \"поверни направо\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSs_w9Lu9i-S"
      },
      "outputs": [],
      "source": [
        "print(\"taking action 2 (right)\")\n",
        "new_state, reward, is_done, truncated, _ = env.step(2)\n",
        "\n",
        "print(\"new state:\", new_state)\n",
        "print(\"reward:\", reward)\n",
        "print(\"is game over?:\", is_done)\n",
        "print(\"is game truncated due to time limit?:\", is_done)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS56PCMa9i-S"
      },
      "source": [
        "### Play with it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrL9_D56VXSI"
      },
      "source": [
        "Ниже приведен код, который поворачивает автомобиль вправо. Однако, если вы просто используете политику по умолчанию, автомобиль не достигнет крайнего правого флажка из-за силы тяжести.\n",
        "\n",
        "__Ваша задача__ - исправить это. Найдите стратегию, которая позволит добраться до флага. \n",
        "\n",
        "На данный момент от вас не требуется создавать какие-либо сложные алгоритмы, и вам определенно не нужно знать какие-либо методы обучения с подкреплением для этого. Не стесняйтесь использовать жесткий код :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CaEYOrf9i-T"
      },
      "outputs": [],
      "source": [
        "actions = {\"left\": 0, \"stop\": 1, \"right\": 2}\n",
        "\n",
        "def policy(state, time_step):\n",
        "    # Write the code for your policy here. You can use the current state\n",
        "    # (a tuple of position and velocity), the current time step, or both,\n",
        "    # if you want.\n",
        "    position, velocity = state\n",
        "    \n",
        "    # Ваш код здесь\n",
        "\n",
        "    return actions[\"right\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k02kqprA9i-T"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output, display\n",
        "time_limit = 250\n",
        "is_done = 0\n",
        "\n",
        "state, _ = env.reset()\n",
        "for time_step in range(time_limit):\n",
        "    plt.gca().clear()\n",
        "    # Choose action based on your policy.\n",
        "\n",
        "    # Pass the action to the environment.\n",
        "    \n",
        "    # We don't do anything with reward here because MountainCar is a very\n",
        "    # simple environment, and reward is a constant -1 (meaning that your\n",
        "    # goal is to end the episode as quickly as possible).\n",
        "    action = policy(state, time_step)\n",
        "    state, reward, is_done, truncated, _ = env.step(action)\n",
        "\n",
        "    # Draw game image on display.\n",
        "    \n",
        "    plt.imshow(env.render())\n",
        "\n",
        "    display(plt.gcf())\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    if is_done:\n",
        "          print(\"Well done!\")\n",
        "          break\n",
        "if not is_done:\n",
        "    print(\"Time limit exceeded. Try again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LYwulaIBZeH"
      },
      "source": [
        "## Crossentropy method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7eHzR_9Bm2K"
      },
      "source": [
        "Теперь, когда мы знаем, как работает `gym`, давайте попробуем решить более сложную задачу, используя метод перекрестной энтропии."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boirRXNq--s-"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
        "env.reset()\n",
        "plt.imshow(env.render())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU66aOiPdVVY"
      },
      "source": [
        "Поскольку `Taxi-v3` представляет собой гораздо более сложную среду, в нашем распоряжении оказывается больше возможных состояний и действий."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9l4J5bpB04k"
      },
      "outputs": [],
      "source": [
        "n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "print(f\"n_states={n_states}, n_actions={n_actions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxLN1ZxmdxcI"
      },
      "source": [
        "Это определенно много. Слишком много для жесткого программирования, как мы делали с предыдущей задачей. Давайте используем метод перекрестной энтропии для решения этой задачи."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FExaEKW-Vf2j"
      },
      "source": [
        "### Create stochastic policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3B63b1fVl6C"
      },
      "source": [
        "На этот раз нашей политикой должно быть распределение вероятностей.\n",
        "\n",
        "`policy[s, a] = P(выполнить действие a | в состоянии s)`\n",
        "\n",
        "Поскольку мы по-прежнему используем целочисленные представления состояния и действия, вы можете использовать двумерный массив для представления политики.\n",
        "\n",
        "Пожалуйста, инициализируйте политику __единообразно__, то есть вероятности всех действий должны быть равны."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_TzXJVHVlY3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialize_policy(n_states, n_actions):\n",
        "    # Create an array to store action probabilities\n",
        "    \n",
        "    # Ваш код здесь\n",
        "    policy = ...\n",
        "    \n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a99xkrpd0pbX"
      },
      "outputs": [],
      "source": [
        "policy = initialize_policy(n_states, n_actions)\n",
        "policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THBVPqvOW2OA"
      },
      "source": [
        "### Play the game"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOgE_1xMXQ61"
      },
      "source": [
        "Давайте поиграем в игру, как и раньше, однако на этот раз мы также будем записывать состояния, действия и награды, чтобы использовать их в цикле тренировок."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.step(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QBRMMZeW38F"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, policy, time_limit=10**4):\n",
        "    state, _ = env.reset()\n",
        "    states, actions = [], []\n",
        "    total_reward = 0.\n",
        "\n",
        "    for _ in range(time_limit):\n",
        "        # Choose action based on policy and take it.\n",
        "        # Ваш код здесь\n",
        "        action = ...\n",
        "        new_state, reward, is_done, truncated, _ = env.step(action)\n",
        "\n",
        "        # Record information we just got from the environment.\n",
        "        # Ваш код здесь\n",
        "        \n",
        "        state = new_state\n",
        "        if is_done:\n",
        "            break\n",
        "\n",
        "    return states, actions, total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFF98gn8Xnme"
      },
      "outputs": [],
      "source": [
        "states, actions, reward = generate_session(env, policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3KlOA04YCpj"
      },
      "source": [
        "Давайте посмотрим на первоначальное распределение вознаграждений за нашу \"наивную\" политику."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP5VOP15X59V"
      },
      "outputs": [],
      "source": [
        "sample_rewards = [generate_session(env, policy, time_limit=1000)[2] for _ in range(200)]\n",
        "plt.hist(sample_rewards, bins=20)\n",
        "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color=\"green\")\n",
        "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color=\"red\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkdNsDUG_jtY"
      },
      "outputs": [],
      "source": [
        "np.percentile(sample_rewards, 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2Hbt1n1YK9G"
      },
      "source": [
        "### Crossentropy method step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5WSk58tYUpp"
      },
      "outputs": [],
      "source": [
        "def select_elites(states_batch, actions_batch, rewards_batch, percentile):\n",
        "    \"\"\"\n",
        "    Select states and actions from games that have rewards >= percentile.\n",
        "\n",
        "    Compute minimum reward for session to be elite and choose elite states\n",
        "    and actions based on this threshold.\n",
        "\n",
        "    Note that states_batch and actions_batch are both 2d lists, i.e. lists\n",
        "    containing lists of states and actions from each session in batch.\n",
        "    \"\"\"\n",
        "    \n",
        "    elite_states = []\n",
        "    elite_actions = []\n",
        "    # Ваш код здесь\n",
        "    \n",
        "    return elite_states, elite_actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMQ00-p6UN3v"
      },
      "outputs": [],
      "source": [
        "def get_new_policy(elite_states, elite_actions):\n",
        "    \"\"\"\n",
        "    Given a list of elite states/actions from select_elites, return a new\n",
        "    policy where each action probability is proportional to\n",
        "\n",
        "        policy[s, a] ~ #[occurrences of s and a in elite states/actions]\n",
        "\n",
        "    Don't forget to normalize the policy to get valid probabilities.\n",
        "    For states that you never visited, use a uniform distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    new_policy = np.ones([n_states, n_actions])\n",
        "\n",
        "    # Set probabilities for actions given elite states & actions.\n",
        "    # Ваш код здесь\n",
        "    \n",
        "    return new_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4SYwLlztaLv"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3E8Jf0ttdUw"
      },
      "source": [
        "Generate sessions, select N best and fit to those."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8ds_27BtEKV"
      },
      "outputs": [],
      "source": [
        "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
        "    \"\"\"\n",
        "    A convenience function that displays training progress. \n",
        "    No cool math here, just charts.\n",
        "    \"\"\"\n",
        "\n",
        "    mean_reward = np.mean(rewards_batch)\n",
        "    threshold = np.percentile(rewards_batch, percentile)\n",
        "    log.append([mean_reward, threshold])\n",
        "    \n",
        "    plt.figure(figsize=[8, 4])\n",
        "    plt.subplot(1, 2, 1)\n",
        "\n",
        "    mean_rewards = [mean_reward for mean_reward, threshold in log]\n",
        "    reward_thresholds = [threshold for mean_reward, threshold in log]\n",
        "    plt.plot(mean_rewards, label=\"Mean rewards\")\n",
        "    plt.plot(reward_thresholds, label=\"Reward thresholds\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(rewards_batch, range=reward_range)\n",
        "    plt.vlines(\n",
        "        [np.percentile(rewards_batch, percentile)],\n",
        "        ymin=[0],\n",
        "        ymax=[100],\n",
        "        label=\"percentile\",\n",
        "        color=\"red\",\n",
        "    )\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    clear_output(wait=True)\n",
        "    print(f\"mean reward = {mean_reward:.3f}, threshold={threshold:.3f}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uh8JpOp0yXL"
      },
      "outputs": [],
      "source": [
        "# reset policy\n",
        "policy = initialize_policy(n_states, n_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWwgfYHB00dE"
      },
      "outputs": [],
      "source": [
        "n_sessions = 250     # sample this many sessions\n",
        "percentile = 70      # take this percent of session with highest rewards\n",
        "learning_rate = 0.8  # how quickly the policy is updated, on a scale from 0 to 1\n",
        "\n",
        "log = []\n",
        "\n",
        "for i in range(40):\n",
        "    # Generate a list of n_sessions new sessions, select elites and compute\n",
        "    # new policy based on them. After that update the existing policy wrt\n",
        "    # learning rate.\n",
        "    sessions = [generate_session(env, policy)  for _ in range(n_sessions)]\n",
        "    states_batch = [session_states for session_states, session_actions, session_reward in  sessions]\n",
        "    actions_batch = [session_actions for session_states, session_actions, session_reward in  sessions]\n",
        "    rewards_batch = [session_reward for session_states, session_actions, session_reward in  sessions]\n",
        "\n",
        "    # Ваш код здесь\n",
        "    \n",
        "    # display results on chart\n",
        "    show_progress(rewards_batch, log, percentile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlIrztK22HqI"
      },
      "source": [
        "### Анализ результатов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hehsmk982J70"
      },
      "source": [
        "Возможно, вы заметили, что проблема с такси быстро переходит от очень низких значений к почти оптимальным, а затем возвращается обратно. Это вызвано (по крайней мере, частично) врожденной случайностью окружающей среды. А именно, исходные точки взаимодействия пассажира и водителя меняются от эпизода к эпизоду.\n",
        "\n",
        "В таком случае, если кроссэнтропийная политика не смогла научиться побеждать с одной четкой отправной точки, она просто откажется от нее, потому что никакие занятия с этой отправной точки не приведут ее в \"элиту\".\n",
        "\n",
        "Чтобы смягчить эту проблему, вы можете либо снизить порог для элитных сессий (метод клейкой ленты), либо изменить способ оценки стратегии (теоретически правильный способ). Для каждого начального состояния вы можете выбрать действие случайным образом, а затем оценить это действие, запустив несколько игр, начиная с него, и усреднив общую награду. Выбор элитных сессий с такой выборкой (где вознаграждение за каждую сессию рассчитывается как среднее значение вознаграждений за все сессии с одинаковым начальным состоянием и действием) должен повысить эффективность вашей политики."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTClEj9c3JrE"
      },
      "source": [
        "## Копаем глубже: приблизительная кроссэнтропия с помощью нейронных сетей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZFlyLJf3TAA"
      },
      "source": [
        "В этом разделе мы расширим вашу реализацию CEM с помощью нейронных сетей! Вы обучите многослойную нейронную сеть решать простые игры с непрерывным пространством состояний.\n",
        "\n",
        "![img](https://watanimg.elwatannews.com/old_news_images/large/249765_Large_20140709045740_11.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS6CtVHR1mRR"
      },
      "outputs": [],
      "source": [
        "# .env is to remove auto-assigned time limit wrapper\n",
        "env = gym.make(\"CartPole-v0\", render_mode='rgb_array').env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "print(\"state vector dim =\", state_dim)\n",
        "print(\"n_actions =\", n_actions)\n",
        "\n",
        "plt.imshow(env.render())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiAvTBy8r9qd"
      },
      "source": [
        "Здесь, как и в ```MountainCar-v0```, мы будем управлять тележкой, которую мы можем перемещать вправо или влево. Однако наша цель здесь иная. В этой среде мы хотим как можно дольше удерживать шест, прикрепленный к верхней части нашей тележки, от падения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udRr7zCW4F9B"
      },
      "source": [
        "### Neural Network Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XXRmz-B4I7L"
      },
      "source": [
        "Для этого задания мы будем использовать упрощенную реализацию нейронной сети из [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). Вот что вам понадобится:\n",
        "* `agent.partial_fit(states, actions)` - выполняет однократный тренировочный проход по данным, чтобы увеличить вероятность выполнения заданных \"actions\" в заданных \"states\".\n",
        "* `agent.predict_proba(states)` - предсказывает вероятности всех действий, используя матрицу вида `[len(states), n_actions] `"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[env.reset()] * n_actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3-zE4Yp3uV8"
      },
      "outputs": [],
      "source": [
        "agent = MLPClassifier(\n",
        "    hidden_layer_sizes=(20, 20),\n",
        "    activation=\"tanh\",\n",
        "    warm_start=True,\n",
        "    max_iter=1\n",
        ")\n",
        "\n",
        "# initialize agent to the dimension of state space and number of actions\n",
        "agent.fit([env.reset()[0]] * n_actions, range(n_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.predict_proba([env.reset()[0]]).squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO3tM6wFtZr_"
      },
      "source": [
        "Несмотря на очевидные различия, вы обнаружите, что процедура обучения для такого агента очень похожа на ту, которую мы использовали в предыдущей части. Нам даже не нужно будет переписывать большинство наших вспомогательных функций! Однако кое-что изменилось - это способ получения вероятностей действий. Итак, давайте адаптируем нашу функцию `generate_session` к этой новой политике, основанной на агентах."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glqypTJL6PmE"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, agent, time_limit=10**4):\n",
        "    state, _ = env.reset()\n",
        "    states, actions = [], []\n",
        "    total_reward = 0.\n",
        "    for _ in range(time_limit):\n",
        "        # Use agent to predict a vector of action probabilities for current \n",
        "        # state and use the probabilities you predicted to pick an action.\n",
        "        # Sample actions, don't just take the most likely one!\n",
        "\n",
        "        # Ваш код здесь\n",
        "        action = ...\n",
        "        \n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        \n",
        "        state, reward, is_done, truncated, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Record information we just got from the environment.\n",
        "        if is_done:\n",
        "            break\n",
        "\n",
        "    return states, actions, total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6-YrqLP6L-1"
      },
      "outputs": [],
      "source": [
        "states, actions, reward = generate_session(env, agent, time_limit=100)\n",
        "print(\"states:\", np.stack(states))\n",
        "print(\"actions:\", actions)\n",
        "print(\"reward:\", reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9cj1aiN7cH-"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFpPGkXL7nCa"
      },
      "outputs": [],
      "source": [
        "n_sessions = 100\n",
        "percentile = 70\n",
        "\n",
        "log = []\n",
        "\n",
        "for _ in range(100):\n",
        "    # Generate new sessions, select elites and update our agent.\n",
        "\n",
        "    sessions = [generate_session(env, agent)  for _ in range(n_sessions)]\n",
        "    states_batch = [session_states for session_states, session_actions, session_reward in  sessions]\n",
        "    actions_batch = [session_actions for session_states, session_actions, session_reward in  sessions]\n",
        "    rewards_batch = [session_reward for session_states, session_actions, session_reward in  sessions]\n",
        "\n",
        "    elite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile)\n",
        "\n",
        "    # Ваш код здесь\n",
        "    ...\n",
        "\n",
        "    show_progress(\n",
        "        rewards_batch, \n",
        "        log, \n",
        "        percentile, \n",
        "        reward_range=[0, np.max(rewards_batch)]\n",
        "    )\n",
        "\n",
        "    if np.mean(rewards_batch) > 190:\n",
        "        print(\"You Win! You may stop training now via KeyboardInterrupt.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3QsLwz38h9h"
      },
      "source": [
        "### Analysing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.RecordVideo(\n",
        "    gym.make(\"CartPole-v0\", render_mode='rgb_array'),\n",
        "    video_folder=\"videos\", \n",
        "    episode_trigger=lambda x: True\n",
        "    ) as env_monitor:\n",
        "    generate_session(env_monitor, agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE8xZc7jTGXS"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "from base64 import b64encode\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_paths = sorted([file for file in Path(\"videos\").iterdir() if file.suffix == \".mp4\"])\n",
        "video_path = video_paths[-1]  # You can also try other indices\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # https://stackoverflow.com/a/57378660/1214547\n",
        "    with video_path.open(\"rb\") as fp:\n",
        "        mp4 = fp.read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "else:\n",
        "    data_url = str(video_path)\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(data_url))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
