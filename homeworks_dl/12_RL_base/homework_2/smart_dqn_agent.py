import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import deque
import game.wrapped_flappy_bird as game # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ game.wrapped_flappy_bird –¥–æ—Å—Ç—É–ø–µ–Ω
import matplotlib.pyplot as plt
import os
import time
import argparse
import datetime # –î–æ–±–∞–≤–∏–º –∏–º–ø–æ—Ä—Ç datetime –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–∏

class SmartDQN(nn.Module):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è DQN —Å dropout –∏ batch normalization"""
    
    def __init__(self, input_size=5, hidden_size=256, output_size=2):
        super(SmartDQN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.bn1 = nn.BatchNorm1d(hidden_size)
        self.dropout1 = nn.Dropout(0.2)
        
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.bn2 = nn.BatchNorm1d(hidden_size)
        self.dropout2 = nn.Dropout(0.2)
        
        self.fc3 = nn.Linear(hidden_size, hidden_size // 2)
        self.fc4 = nn.Linear(hidden_size // 2, output_size)
        
    def forward(self, x):
        # BatchNorm1d –æ–∂–∏–¥–∞–µ—Ç (N, C) –∏–ª–∏ (N, C, L), –≥–¥–µ N - batch_size, C - channels (—Ñ–∏—á–∏)
        # –ï—Å–ª–∏ x –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π (—Ç–æ–ª—å–∫–æ —Ñ–∏—á–∏), –∏ –±–∞—Ç—á —Ä–∞–∑–º–µ—Ä–æ–º 1, –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–∞
        if x.ndim == 1:
            x = x.unsqueeze(0) # (features) -> (1, features) - –¥–ª—è —Å–ª—É—á–∞—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –æ–¥–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        
        # –ï—Å–ª–∏ –±–∞—Ç—á —É–∂–µ –µ—Å—Ç—å, –Ω–æ –Ω–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∫–∞–Ω–∞–ª–∞ (N, features)
        # BatchNorm1d –æ–∂–∏–¥–∞–µ—Ç (N, num_features)
        # Linear –æ–∂–∏–¥–∞–µ—Ç (N, *, in_features)

        x = torch.relu(self.bn1(self.fc1(x)))
        x = self.dropout1(x)
        x = torch.relu(self.bn2(self.fc2(x)))
        x = self.dropout2(x)
        x = torch.relu(self.fc3(x))
        return self.fc4(x)


class SmartAgent:
    """–£–º–Ω—ã–π –∞–≥–µ–Ω—Ç —Å curriculum learning –∏ shaped rewards"""
    
    def __init__(self, state_size=5, action_size=2, lr=0.001):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=50000)
        self.epsilon = 0.9  # –ù–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ epsilon
        self.epsilon_min = 0.001
        self.epsilon_decay = 0.9999 # –ó–∞–º–µ–¥–ª—è–µ–º –∑–∞—Ç—É—Ö–∞–Ω–∏–µ epsilon –µ—â–µ –±–æ–ª—å—à–µ
        self.learning_rate = lr
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.q_network = SmartDQN(state_size, 256, action_size).to(self.device)
        self.target_network = SmartDQN(state_size, 256, action_size).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr, weight_decay=1e-4) # –î–æ–±–∞–≤–ª–µ–Ω weight_decay –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
        
        self.update_target_network()
        
    def update_target_network(self):
        self.target_network.load_state_dict(self.q_network.state_dict())
        
    def get_enhanced_state(self, env):
        bird_y = env.playery / float(game.SCREENHEIGHT) # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        bird_vel = env.playerVelY / 15.0 # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–ø—Ä–∏–º–µ—Ä–Ω–∞—è)
        
        next_pipe_dist_norm = 1.0
        next_pipe_gap_center_norm = 0.5 
        next_pipe_gap_top_norm = 0.5 # –¶–µ–Ω—Ç—Ä —ç–∫—Ä–∞–Ω–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –µ—Å–ª–∏ —Ç—Ä—É–± –Ω–µ—Ç

        bird_center_x_abs = env.playerx + game.PLAYER_WIDTH / 2.0
        
        closest_pipe = None
        min_dist = float('inf')

        for upper_pipe in env.upperPipes:
            pipe_right_edge = upper_pipe['x'] + game.PIPE_WIDTH
            if pipe_right_edge > bird_center_x_abs: # –¢—Ä—É–±–∞ –µ—â–µ –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ–π–¥–µ–Ω–∞ —Ü–µ–Ω—Ç—Ä–æ–º –ø—Ç–∏—Ü—ã
                dist_to_pipe_front = upper_pipe['x'] - bird_center_x_abs
                if dist_to_pipe_front < min_dist : # –ò—â–µ–º –±–ª–∏–∂–∞–π—à—É—é —Ç—Ä—É–±—É –°–ø–µ—Ä–µ–¥–∏
                    # –£—Å–ª–æ–≤–∏–µ dist_to_pipe_front > -game.PIPE_WIDTH/2 –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º—ã –µ—â–µ –Ω–µ –≥–ª—É–±–æ–∫–æ –≤–Ω—É—Ç—Ä–∏ —Ç—Ä—É–±—ã
                    if dist_to_pipe_front > -game.PIPE_WIDTH : # –ß—Ç–æ–±—ã –Ω–µ –±—Ä–∞–ª —Ç—É —Ç—Ä—É–±—É, –∫–æ—Ç–æ—Ä—É—é —Ç–æ–ª—å–∫–æ —á—Ç–æ –ø—Ä–æ—à–ª–∏
                        min_dist = dist_to_pipe_front
                        closest_pipe = upper_pipe


        if closest_pipe:
            # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –Ω–∏–∂–Ω—é—é —Ç—Ä—É–±—É (–æ–Ω–∏ —Ö—Ä–∞–Ω—è—Ç—Å—è –ø–∞—Ä–∞–º–∏ –≤ self.upperPipes –∏ self.lowerPipes)
            # –≠—Ç–æ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ —Ç—Ä—É–±—ã –≤—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –∏ —É–¥–∞–ª—è—é—Ç—Å—è –ø–∞—Ä–∞–º–∏.
            try:
                pipe_index = -1
                for i, p in enumerate(env.upperPipes):
                    if p['x'] == closest_pipe['x'] and p['y'] == closest_pipe['y']:
                        pipe_index = i
                        break
                
                if pipe_index != -1:
                    lower_pipe_for_closest_upper = env.lowerPipes[pipe_index]

                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –Ω–∞—á–∞–ª–∞ —Ç—Ä—É–±—ã (0, –µ—Å–ª–∏ –ø—Ä—è–º–æ –ø–µ—Ä–µ–¥ –Ω–µ–π, 1 –µ—Å–ª–∏ –¥–∞–ª–µ–∫–æ)
                    # SCREENWIDTH - —ç—Ç–æ –ø—Ä–∏–º–µ—Ä–Ω—ã–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–∏–¥–∏–º—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è —Ç—Ä—É–±
                    next_pipe_dist_norm = max(0, min_dist) / float(game.SCREENWIDTH) 
                    
                    gap_top_y = closest_pipe['y'] + game.PIPE_HEIGHT # y –≤–µ—Ä—Ö–Ω–µ–≥–æ –∫—Ä–∞—è –Ω–∏–∂–Ω–µ–π —á–∞—Å—Ç–∏ —Ç—Ä—É–±—ã (—Ä–µ–∞–ª—å–Ω—ã–π –≤–µ—Ä—Ö –∑–∞–∑–æ—Ä–∞)
                    gap_bottom_y = lower_pipe_for_closest_upper['y'] # y –≤–µ—Ä—Ö–Ω–µ–≥–æ –∫—Ä–∞—è –Ω–∏–∂–Ω–µ–π —Ç—Ä—É–±—ã (—Ä–µ–∞–ª—å–Ω—ã–π –Ω–∏–∑ –∑–∞–∑–æ—Ä–∞)
                    
                    gap_center_y = (gap_top_y + gap_bottom_y) / 2.0
                    
                    next_pipe_gap_center_norm = gap_center_y / float(game.SCREENHEIGHT)
                    next_pipe_gap_top_norm = gap_top_y / float(game.SCREENHEIGHT) # –ü–æ–∑–∏—Ü–∏—è –≤–µ—Ä—Ö–Ω–µ–≥–æ –∫—Ä–∞—è –∑–∞–∑–æ—Ä–∞
                
            except IndexError: # –ù–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ (–º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–µ –∏–≥—Ä—ã)
                 pass


        return np.array([bird_y, bird_vel, next_pipe_dist_norm, next_pipe_gap_center_norm, next_pipe_gap_top_norm])

    def get_shaped_reward(self, env, terminal, prev_score, state_info):
        # state_info —ç—Ç–æ bird_y, bird_vel, next_pipe_dist_norm, next_pipe_gap_center_norm
        bird_y_norm, _, next_pipe_dist_norm, next_pipe_gap_center_norm, next_pipe_gap_top_norm = state_info
        
        reward = 0
        
        # –ë–∞–∑–æ–≤–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ –≤—ã–∂–∏–≤–∞–Ω–∏–µ (–Ω–µ–±–æ–ª—å—à–∞—è, —á—Ç–æ–±—ã –Ω–µ –ø–æ–æ—â—Ä—è—Ç—å –ø—Ä–æ—Å—Ç–æ–µ –ø–∞–¥–µ–Ω–∏–µ)
        # reward += 0.01 # –£–±—Ä–∞–ª, —Ç.–∫. –µ—Å—Ç—å –≤ env.frame_step

        if terminal:
            return -100  # –ë–æ–ª—å—à–æ–µ –Ω–∞–∫–∞–∑–∞–Ω–∏–µ –∑–∞ —Å–º–µ—Ä—Ç—å

        # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –∫–∞–∂–¥—ã–π –≤—ã–∂–∏–≤—à–∏–π –∫–∞–¥—Ä (–ø–æ–º–æ–≥–∞–µ—Ç –∞–≥–µ–Ω—Ç—É –Ω–∞—É—á–∏—Ç—å—Å—è –¥–æ–ª—å—à–µ –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –≤ –∂–∏–≤—ã—Ö)
        reward += 1.0 

        if env.score > prev_score:
            return 200  # –û—á–µ–Ω—å –±–æ–ª—å—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ –æ—á–∫–∏ (–ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ —Ç—Ä—É–±—ã)

        # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ –∫ —Ü–µ–Ω—Ç—Ä—É –∑–∞–∑–æ—Ä–∞ —Å–ª–µ–¥—É—é—â–µ–π —Ç—Ä—É–±—ã
        # –ß–µ–º –±–ª–∏–∂–µ –ø—Ç–∏—Ü–∞ –∫ —Ü–µ–Ω—Ç—Ä—É –∑–∞–∑–æ—Ä–∞ –ò —á–µ–º –±–ª–∏–∂–µ —Å–∞–º–∞ —Ç—Ä—É–±–∞, —Ç–µ–º –≤—ã—à–µ –Ω–∞–≥—Ä–∞–¥–∞
        # next_pipe_dist_norm: 0 (–±–ª–∏–∑–∫–æ) to 1 (–¥–∞–ª–µ–∫–æ)
        # bird_y_norm: –ø–æ–ª–æ–∂–µ–Ω–∏–µ –ø—Ç–∏—Ü—ã
        # next_pipe_gap_center_norm: —Ü–µ–Ω—Ç—Ä –∑–∞–∑–æ—Ä–∞
        
        # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ü–µ–Ω—Ç—Ä–∞ –∑–∞–∑–æ—Ä–∞ –ø–æ –≤–µ—Ä—Ç–∏–∫–∞–ª–∏
        vertical_dist_to_gap_center = abs(bird_y_norm - next_pipe_gap_center_norm)

        if next_pipe_dist_norm < 0.3: # –ï—Å–ª–∏ —Ç—Ä—É–±–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–ª–∏–∑–∫–æ
            # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –∑–æ–Ω–µ –∑–∞–∑–æ—Ä–∞
            # next_pipe_gap_top_norm - —ç—Ç–æ –≤–µ—Ä—Ö–Ω–∏–π –∫—Ä–∞–π –∑–∞–∑–æ—Ä–∞ (–Ω–∏–∑ –≤–µ—Ä—Ö–Ω–µ–π —Ç—Ä—É–±—ã)
            # next_pipe_gap_bottom_norm –º–æ–∂–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –∫–∞–∫ next_pipe_gap_top_norm + (PIPEGAPSIZE / SCREENHEIGHT)
            pipe_gap_size_norm = game.PIPEGAPSIZE / float(game.SCREENHEIGHT)
            gap_bottom_actual_norm = next_pipe_gap_top_norm + pipe_gap_size_norm

            # –ï—Å–ª–∏ –ø—Ç–∏—Ü–∞ –º–µ–∂–¥—É –≤–µ—Ä—Ö–æ–º –∏ –Ω–∏–∑–æ–º –∑–∞–∑–æ—Ä–∞
            if next_pipe_gap_top_norm < bird_y_norm < gap_bottom_actual_norm:
                # –ß–µ–º –±–ª–∏–∂–µ –∫ —Ü–µ–Ω—Ç—Ä—É, —Ç–µ–º –ª—É—á—à–µ. max reward = 10
                reward += (1.0 - (vertical_dist_to_gap_center / (pipe_gap_size_norm / 2.0))) * 10 
            else:
                # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –≤–Ω–µ –∑–∞–∑–æ—Ä–∞, –∫–æ–≥–¥–∞ —Ç—Ä—É–±–∞ –±–ª–∏–∑–∫–æ
                reward -= 5
        
        # –ù–µ–±–æ–ª—å—à–æ–π —à—Ç—Ä–∞—Ñ –∑–∞ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –±–ª–∏–∑–∫–æ –∫ –∑–µ–º–ª–µ –∏–ª–∏ –ø–æ—Ç–æ–ª–∫—É, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ç—Ä—É–±
        if bird_y_norm < 0.1 or bird_y_norm > 0.9: # 10% –æ—Ç –≤–µ—Ä—Ö–∞/–Ω–∏–∑–∞ —ç–∫—Ä–∞–Ω–∞
            reward -= 2
        elif bird_y_norm < 0.15 or bird_y_norm > 0.85: # 15% –æ—Ç –≤–µ—Ä—Ö–∞/–Ω–∏–∑–∞ —ç–∫—Ä–∞–Ω–∞
            reward -= 1
            
        return reward
        
    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
        
    def act(self, state):
        if np.random.random() <= self.epsilon:
            return random.randrange(self.action_size)
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        self.q_network.eval() # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏
        with torch.no_grad():
            q_values = self.q_network(state_tensor)
        self.q_network.train() # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
        return np.argmax(q_values.cpu().data.numpy())
        
    def replay(self, batch_size=64):
        if len(self.memory) < batch_size:
            return 0 # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 0 –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ü–∏–∏, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ –ø—Ä–æ–∏–∑–æ—à–ª–æ
            
        batch = random.sample(self.memory, batch_size)
        states = torch.FloatTensor(np.array([e[0] for e in batch])).to(self.device)
        actions = torch.LongTensor([e[1] for e in batch]).unsqueeze(1).to(self.device) # unsqueeze –¥–ª—è gather
        rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)
        next_states = torch.FloatTensor(np.array([e[3] for e in batch])).to(self.device)
        dones = torch.BoolTensor([e[4] for e in batch]).to(self.device)
        
        current_q_values = self.q_network(states).gather(1, actions)
        
        with torch.no_grad():
            next_q_values_target = self.target_network(next_states).max(1)[0]
            # –ï—Å–ª–∏ —Å–ª–µ–¥—É—é—â–∏–π —Å—Ç–µ–π—Ç —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω—ã–π, —Ç–æ –µ–≥–æ —Ü–µ–Ω–Ω–æ—Å—Ç—å 0
            next_q_values_target[dones] = 0.0 
        
        target_q_values = rewards + (0.99 * next_q_values_target) # gamma = 0.99
        
        loss = nn.SmoothL1Loss()(current_q_values.squeeze(), target_q_values) # squeeze current_q_values
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0) # Gradient clipping
        self.optimizer.step()
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        return loss.item() # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
            
    def save(self, name):
        torch.save({
            'model_state_dict': self.q_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon
        }, name)
        print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {name}")
        
    def load(self, name):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª
        if not os.path.exists(name):
            print(f"–û—à–∏–±–∫–∞: –§–∞–π–ª –º–æ–¥–µ–ª–∏ {name} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            return False
        try:
            checkpoint = torch.load(name, map_location=self.device) # map_location –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏ cpu/gpu
            self.q_network.load_state_dict(checkpoint['model_state_dict'])
            self.target_network.load_state_dict(checkpoint['model_state_dict']) # –¢–∞–∫–∂–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ target_network
            if 'optimizer_state_dict' in checkpoint: # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –µ—Å–ª–∏ —Ä–∞–Ω–µ–µ –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–ª–∏
                 self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            if 'epsilon' in checkpoint:
                 self.epsilon = checkpoint['epsilon']
            print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {name}")
            return True
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ {name}: {e}")
            return False


def train_smart_agent(episodes=3000, lr=0.00025, model_load_path=None):
    env = game.GameState()
    agent = SmartAgent(state_size=5, action_size=2, lr=lr)
    
    if model_load_path:
        if not agent.load(model_load_path):
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {model_load_path}, –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.")

    target_update_freq = 10  # –û–±–Ω–æ–≤–ª—è–µ–º target network –∫–∞–∂–¥—ã–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤ (–±—ã–ª–æ 50, –Ω–æ –º–æ–∂–Ω–æ —á–∞—â–µ –ø—Ä–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö —ç–ø–∏–∑–æ–¥–∞—Ö)
    replay_start_size = 2000 # –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ N —à–∞–≥–æ–≤ –≤ –ø–∞–º—è—Ç–∏ (—É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è)
    batch_size = 64

    scores_history = []
    episode_rewards_history = []
    losses_history = []
    max_score_achieved = 0
    
    os.makedirs('models', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    
    print(f"–û–±—É—á–µ–Ω–∏–µ SmartDQN –∞–≥–µ–Ω—Ç–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {agent.device}")
    start_time = time.time()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∞–π–ª–æ–≤
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    for episode in range(1, episodes + 1):
        env.__init__() # –°–±—Ä–æ—Å –∏–≥—Ä—ã
        current_state_info = agent.get_enhanced_state(env)
        total_episode_reward = 0
        current_score = 0
        steps_in_episode = 0
        episode_loss_sum = 0
        episode_loss_count = 0
        
        while True:
            steps_in_episode += 1
            action = agent.act(current_state_info)
            
            action_input = [0, 0]
            if action == 1: # flap
                action_input[1] = 1
            else: # do nothing
                action_input[0] = 1
                
            _, env_reward, terminal = env.frame_step(action_input) # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –∞–≥–µ–Ω—Ç–æ–º
            
            next_state_info = agent.get_enhanced_state(env)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –Ω–∞–≥—Ä–∞–¥—ã –∞–≥–µ–Ω—Ç–∞
            # –ü–µ—Ä–µ–¥–∞–µ–º env, terminal, prev_score (current_score –¥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è), next_state_info
            shaped_reward = agent.get_shaped_reward(env, terminal, current_score, next_state_info)
            total_episode_reward += shaped_reward # –°—É–º–º–∏—Ä—É–µ–º "—É–º–Ω—ã–µ" –Ω–∞–≥—Ä–∞–¥—ã
            
            agent.remember(current_state_info, action, shaped_reward, next_state_info, terminal)
            current_state_info = next_state_info
            current_score = env.score # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç –ø–æ—Å–ª–µ —à–∞–≥–∞
            
            if len(agent.memory) > replay_start_size:
                loss = agent.replay(batch_size)
                if loss > 0: # –ï—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–æ—à–ª–æ
                    episode_loss_sum += loss
                    episode_loss_count += 1
            
            if terminal or steps_in_episode > 7000: # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ —à–∞–≥–∏ –≤ —ç–ø–∏–∑–æ–¥–µ
                break
        
        scores_history.append(current_score)
        episode_rewards_history.append(total_episode_reward)
        if episode_loss_count > 0:
            losses_history.append(episode_loss_sum / episode_loss_count)
        else:
            losses_history.append(None) # –ï—Å–ª–∏ –Ω–µ –±—ã–ª–æ –æ–±—É—á–µ–Ω–∏—è –≤ —ç—Ç–æ–º —ç–ø–∏–∑–æ–¥–µ
        
        if episode % target_update_freq == 0:
            agent.update_target_network()
        
        if current_score > max_score_achieved:
            max_score_achieved = current_score
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É –∫ –Ω–∞–∑–≤–∞–Ω–∏—é —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏
            agent.save(f'models/smart_agent_best_score_{max_score_achieved}_{timestamp}.pt')
            print(f"üéâ –ù–æ–≤—ã–π —Ä–µ–∫–æ—Ä–¥! –°—á–µ—Ç: {max_score_achieved} (—ç–ø–∏–∑–æ–¥ {episode})")
        
        avg_score_last_100 = np.mean(scores_history[-100:]) if len(scores_history) >= 100 else np.mean(scores_history)
        avg_reward_last_100 = np.mean(episode_rewards_history[-100:]) if len(episode_rewards_history) >= 100 else np.mean(episode_rewards_history)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞
        print(f"–≠–ø {episode:5d} | –°—á–µ—Ç: {current_score:3d} | –°—Ä.—Å—á–µ—Ç(100): {avg_score_last_100:6.2f} | "
                  f"–ú–∞–∫—Å: {max_score_achieved:3d} | Epsilon: {agent.epsilon:.4f} | "
                  f"–°—Ä.–Ω–∞–≥—Ä–∞–¥–∞(100): {avg_reward_last_100:8.2f} | –®–∞–≥–∏: {steps_in_episode:4d}")
        
        if max_score_achieved >= 100: # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –ø–æ –î–ó
            print(f"üèÜ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê! –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å—á–µ—Ç: {max_score_achieved} –≤ —ç–ø–∏–∑–æ–¥–µ {episode}.")
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É –∫ –Ω–∞–∑–≤–∞–Ω–∏—é —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏
            agent.save(f'models/smart_agent_target_reached_100_{timestamp}.pt')
            # –ú–æ–∂–Ω–æ —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å break, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è –ø–æ—Å–ª–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏
            # break 
            
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
    agent.save(f'models/smart_agent_final_{timestamp}.pt')
    total_training_time = time.time() - start_time
    print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_training_time/60:.2f} –º–∏–Ω—É—Ç.")
    
    # –ì—Ä–∞—Ñ–∏–∫–∏
    plt.figure(figsize=(18, 10))
    
    plt.subplot(2, 2, 1)
    plt.plot(scores_history)
    plt.title('–°—á–µ—Ç –ø–æ —ç–ø–∏–∑–æ–¥–∞–º')
    plt.xlabel('–≠–ø–∏–∑–æ–¥')
    plt.ylabel('–°—á–µ—Ç')
    if len(scores_history) >= 100:
        moving_avg_scores = np.convolve(scores_history, np.ones(100)/100, mode='valid')
        plt.plot(np.arange(99, len(scores_history)), moving_avg_scores, label='–°—Ä. —Å—á–µ—Ç (100 —ç–ø.)', color='red')
        plt.legend()

    plt.subplot(2, 2, 2)
    plt.plot(episode_rewards_history)
    plt.title('–û–±—â–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —ç–ø–∏–∑–æ–¥')
    plt.xlabel('–≠–ø–∏–∑–æ–¥')
    plt.ylabel('–û–±—â–∞—è –Ω–∞–≥—Ä–∞–¥–∞')
    if len(episode_rewards_history) >= 100:
        moving_avg_rewards = np.convolve(episode_rewards_history, np.ones(100)/100, mode='valid')
        plt.plot(np.arange(99, len(episode_rewards_history)), moving_avg_rewards, label='–°—Ä. –Ω–∞–≥—Ä–∞–¥–∞ (100 —ç–ø.)', color='red')
        plt.legend()
        
    plt.subplot(2, 2, 3)
    valid_losses = [l for l in losses_history if l is not None]
    valid_indices = [i for i, l in enumerate(losses_history) if l is not None]
    if valid_losses:
        plt.plot(valid_indices, valid_losses)
        plt.title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (Loss) –ø–æ —ç–ø–∏–∑–æ–¥–∞–º (–≥–¥–µ –±—ã–ª–æ –æ–±—É—á–µ–Ω–∏–µ)')
        plt.xlabel('–≠–ø–∏–∑–æ–¥')
        plt.ylabel('Loss')

    plt.subplot(2, 2, 4)
    epsilons = [agent.epsilon_min + (0.9 - agent.epsilon_min) * (agent.epsilon_decay**i) for i in range(episode)]
    if episode > 0 : # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –±—ã–ª —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —ç–ø–∏–∑–æ–¥
        actual_epsilons_at_log_points = []
        initial_epsilon = 0.9 # –∏–ª–∏ model_load_path –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∂–∞–ª–∏
        current_epsilon_val = initial_epsilon
        # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —ç–ø—Å–∏–ª–æ–Ω, –µ—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –±—ã–ª–æ –Ω–µ —Å –Ω—É–ª—è
        # –ë–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —ç–ø—Å–∏–ª–æ–Ω
        # –≠—Ç–æ—Ç –≥—Ä–∞—Ñ–∏–∫ –±—É–¥–µ—Ç —Å–∫–æ—Ä–µ–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º —É–±—ã–≤–∞–Ω–∏–µ–º —ç–ø—Å–∏–ª–æ–Ω
        temp_eps = agent.epsilon if model_load_path else 0.9 
        eps_decay_rate = agent.epsilon_decay
        eps_min_val = agent.epsilon_min
        recorded_eps = []
        for i in range(1, episode + 1):
            recorded_eps.append(temp_eps)
            if len(agent.memory) > replay_start_size : # –≠–ø—Å–∏–ª–æ–Ω —É–±—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –Ω–∞—á–∞–ª–∞ replay
                 if temp_eps > eps_min_val:
                    temp_eps *= eps_decay_rate

        plt.plot(recorded_eps)
        plt.title('Epsilon Decay')
        plt.xlabel('–≠–ø–∏–∑–æ–¥')
        plt.ylabel('Epsilon')
    
    plt.tight_layout()
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É –∫ –Ω–∞–∑–≤–∞–Ω–∏—é —Ñ–∞–π–ª–∞ –≥—Ä–∞—Ñ–∏–∫–∞
    plt.savefig(f'logs/smart_agent_training_report_{timestamp}.png', dpi=150)
    print(f"–ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ logs/smart_agent_training_report_{timestamp}.png")
    # plt.show() # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ, –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø–æ–∫–∞–∑–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ —Å—Ä–∞–∑—É

    print(f"\n{'='*50}")
    print(f"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø SMARTDQN –ê–ì–ï–ù–¢–ê")
    print(f"{'='*50}")
    print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å—á–µ—Ç: {max_score_achieved}")
    print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ä–µ–¥–Ω–∏–π —Å—á–µ—Ç (100 —ç–ø–∏–∑–æ–¥–æ–≤): {avg_score_last_100:.2f}")
    print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤: {episode}")
    print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π epsilon: {agent.epsilon:.4f}")
    
    return agent, scores_history


def test_smart_agent(model_path, num_games=10):
    agent = SmartAgent(state_size=5, action_size=2)
    if not agent.load(model_path):
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {model_path} –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.")
        return []
        
    agent.epsilon = 0  # –û—Ç–∫–ª—é—á–∞–µ–º exploration –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    
    env = game.GameState()
    scores = []
    
    print(f"\n–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å: {model_path}")
    
    for game_num in range(1, num_games + 1):
        env.__init__()
        state = agent.get_enhanced_state(env)
        steps = 0
        current_game_score = 0
        
        while True:
            steps += 1
            action = agent.act(state)
            
            action_input = [0,0]
            if action == 1: action_input[1] = 1
            else: action_input[0] = 1
            
            _, _, terminal = env.frame_step(action_input) # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –Ω–∞–≥—Ä–∞–¥–∞ —Å—Ä–µ–¥—ã –Ω–µ –Ω—É–∂–Ω—ã –¥–ª—è —Ç–µ—Å—Ç–∞
            state = agent.get_enhanced_state(env)
            current_game_score = env.score
            
            if terminal or steps > 10000: # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ —à–∞–≥–∏ –≤ –∏–≥—Ä–µ
                break
        
        scores.append(current_game_score)
        print(f"–ò–≥—Ä–∞ {game_num:2d}: –°—á–µ—Ç = {current_game_score:3d}, –®–∞–≥–æ–≤ = {steps:4d}")
    
    avg_score = np.mean(scores) if scores else 0
    max_s = max(scores) if scores else 0
    min_s = min(scores) if scores else 0
    
    print(f"\n{'='*40}")
    print(f"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ({num_games} –∏–≥—Ä)")
    print(f"{'='*40}")
    print(f"–°—Ä–µ–¥–Ω–∏–π —Å—á–µ—Ç: {avg_score:.2f}")
    print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å—á–µ—Ç: {max_s}")
    print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å—á–µ—Ç: {min_s}")
    if scores:
        print(f"–ü—Ä–æ—Ü–µ–Ω—Ç –∏–≥—Ä —Å —Å—á–µ—Ç–æ–º >= 10: {sum(1 for s in scores if s >= 10) / len(scores) * 100:.1f}%")
        print(f"–ü—Ä–æ—Ü–µ–Ω—Ç –∏–≥—Ä —Å —Å—á–µ—Ç–æ–º >= 100: {sum(1 for s in scores if s >= 100) / len(scores) * 100:.1f}%")
    print(f"–í—Å–µ —Å—á–µ—Ç–∞: {scores}")
    
    return scores


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='SmartDQN –∞–≥–µ–Ω—Ç –¥–ª—è Flappy Bird')
    parser.add_argument('--train', action='store_true', help='–û–±—É—á–∏—Ç—å –Ω–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞')
    parser.add_argument('--test', type=str, metavar='MODEL_PATH', help='–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å (—É–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ .pt —Ñ–∞–π–ª—É)')
    parser.add_argument('--load_and_train', type=str, metavar='MODEL_PATH', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ')
    parser.add_argument('--episodes', type=int, default=3000, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 3000)')
    parser.add_argument('--lr', type=float, default=0.00025, help='–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate) (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.00025)')
    parser.add_argument('--games', type=int, default=10, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–≥—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)')
    
    args = parser.parse_args()
    
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø–∞–ø–∫–∞ game —Å wrapped_flappy_bird.py –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ PYTHONPATH –∏–ª–∏ –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    # sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))) # –ü—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ —Å–∫—Ä–∏–ø—Ç –≤ –ø–æ–¥–ø–∞–ø–∫–µ
    
    if args.train:
        print("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞...")
        train_smart_agent(episodes=args.episodes, lr=args.lr)
    elif args.load_and_train:
        print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ {args.load_and_train} –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
        train_smart_agent(episodes=args.episodes, lr=args.lr, model_load_path=args.load_and_train)
    elif args.test:
        test_smart_agent(model_path=args.test, num_games=args.games)
    else:
        print("–ù–µ —É–∫–∞–∑–∞–Ω —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --train –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, "
              "--load_and_train <path> –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è, "
              "–∏–ª–∏ --test <path> –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏.")
        parser.print_help()