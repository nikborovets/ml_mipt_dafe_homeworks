# Неделя 5: Attention

* Лекция: [слайды](https://github.com/ml-dafe/ml_mipt_dafe/blob/main/05_Attention/lec5.pdf), [запись]()
* Семинар: [содержание](https://github.com/ml-dafe/ml_mipt_dafe/blob/main/05_Attention/seminar/Transformers_solved.ipynb), [запись]()
* Домашняя работа: [описание](https://github.com/ml-dafe/ml_mipt_dafe/blob/main/05_Attention/homework/readme.md)

## Литература

* Illustrated transformer [post](https://jalammar.github.io/illustrated-transformer/)
* Distill.pub post on attention and augmentations for RNN - [post](https://distill.pub/2016/augmented-rnns/)
* Seq2seq lecture - [video](https://www.youtube.com/watch?v=G5RY_SUJih4)
* [BLEU](http://www.aclweb.org/anthology/P02-1040.pdf) and [CIDEr](https://arxiv.org/pdf/1411.5726.pdf) articles.
* Image captioning
  * MSCOCO captioning [challenge](http://mscoco.org/dataset/#captions-challenge2015)
  * Captioning baseline [notebook](https://github.com/yandexdataschool/Practical_DL/tree/fall18/week07_seq2seq)
* [NLP Course For You](https://lena-voita.github.io/nlp_course.html#preview_seq2seq_attn) 
